{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db88b6f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvModel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflappy_bird\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "File \u001b[0;32m~/Desktop/comp9444/notebook/LastBus-Project/src/flappy_bird.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycle\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rect, init, time, display\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pump\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.memory import *\n",
    "from src.convModel import *\n",
    "from src.utils import *\n",
    "from src.flappy_bird import *\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import time as t\n",
    "\n",
    "# train_model will train model 2 million times and save model and graph every 100k times\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    action = torch.zeros(2, dtype=torch.float32)\n",
    "    iter = 0\n",
    "    state_image, reward, terminal = game.next_frame(action[1])\n",
    "    state = pre_processing(state_image)\n",
    "    state = torch.cat((state, state, state, state)).unsqueeze(0)\n",
    "    alive_stat = []\n",
    "    alive_time = 0\n",
    "    log_path = \"tensorboard\"\n",
    "    save_path = \"trained_models\"\n",
    "    graph_path = \"graph\"\n",
    "    if os.path.isdir(log_path):\n",
    "        shutil.rmtree(log_path)\n",
    "    os.makedirs(log_path)\n",
    "    while iter < MAX_ITER:\n",
    "        iter = iter + 1\n",
    "        q_value = model(state)[0]\n",
    "        \n",
    "        action = torch.zeros(2, dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            action = action.cuda()\n",
    "        \n",
    "        # Epsilon-Greedy implementation\n",
    "        epsilon = INITIAL_EPSILON * (1 - iter / MAX_ITER)\n",
    "        u = random.random()\n",
    "        random_action = False if epsilon < u else True\n",
    "        index = torch.argmax(q_value).item()\n",
    "        \n",
    "        if random_action:\n",
    "            print(\"Performed random action!\")\n",
    "            index = randint(0, 1)     \n",
    "        action[index] = 1  \n",
    "       \n",
    "        next_state_image, reward, terminal = game.next_frame(action[1])\n",
    "        next_state_image = pre_processing(next_state_image)\n",
    "        next_state = torch.cat((state.squeeze(0)[1:, :, :], next_state_image)).unsqueeze(0)\n",
    "        \n",
    "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "        \n",
    "        action = action.unsqueeze(0)\n",
    "        \n",
    "        # save replay\n",
    "        memory.push(state, action, next_state, reward, terminal)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        update_model()\n",
    "\n",
    "        print(\"Iteration: {}/{}, Action: {}, Reward: {}, Q-value: {}\".format(\n",
    "            iter + 1,\n",
    "            MAX_ITER,\n",
    "            action[0],\n",
    "            reward, torch.max(q_value)))\n",
    "\n",
    "        alive_time += 1\n",
    "        if(iter+1) % 100000 == 0:\n",
    "            torch.save(model, \"{}/flappy_bird_{}\".format(save_path, iter+1))\n",
    "        \n",
    "        if terminal:\n",
    "            alive_stat.append(alive_time)\n",
    "            plot_duration(alive_stat)\n",
    "    \n",
    "            alive_time = 0\n",
    "        if(iter+1) % 100000 == 0:\n",
    "            plt.savefig('{}/train_{}.jpg'.format(graph_path, iter+1))\n",
    "    torch.save(model,\"{}/flappy_bird\".format(save_path))\n",
    "\n",
    "\n",
    "\n",
    "def update_model():\n",
    "    \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    # unpack minibatch\n",
    "    state_batch = torch.cat(tuple(d[0] for d in batch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in batch))\n",
    "    next_state_batch = torch.cat(tuple(d[2] for d in batch))\n",
    "    reward_batch = torch.cat(tuple(d[3] for d in batch))\n",
    "    terminal_batch =[d[4] for d in batch]\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        next_state_batch = next_state_batch.cuda()\n",
    "        terminal_batch = terminal_batch.cuda()\n",
    "\n",
    "    next_action_batch = model(next_state_batch)\n",
    "    # if dead, rj, otherwise r_j + gamma*max(Q_t+1)\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if batch[i][4]\n",
    "                                  else reward_batch[i] + DISCOUNT_FACTOR * torch.max(next_action_batch[i])\n",
    "                                  for i in range(len(batch))))\n",
    "    # Extract Q-value (this part i don't understand)\n",
    "    \n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(q_value, y_batch)\n",
    "\n",
    "    # Do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "def plot_duration(duration):\n",
    "    \"\"\"Plot durations of episodes and average over last 100 episodes\"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(duration, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    LEARNING_RATE = 1e-5\n",
    "    MAX_ITER = 500000\n",
    "    MAX_EXPERIENCE = 50\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    BATCH_SIZE = 30\n",
    "    INITIAL_EPSILON = 0.2\n",
    "    \n",
    "    start_time = t.time()\n",
    "    iter = 0\n",
    "    \n",
    "    # Constructing memory class\n",
    "    memory = Memory(MAX_EXPERIENCE)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    game = FlappyBird()\n",
    "    model = ThreeLayerConvModel()\n",
    "    optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "    graph_path = \"graph\"\n",
    "    plt.ion()\n",
    "    train_model()\n",
    "    print(t.time() - start_time)\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587295b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
