{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27586ebf",
   "metadata": {},
   "source": [
    "This project aims to study and select the best experience replay method of deep-q-learning in training agent to play the game \"Flappy Bird\", by experimenting the reward received under different hyperparameters used, ranging from initial epsilon, to learning rate, and memory size used to update the 3 layer convolution fully connected neural network. Agent first receive environment information through image per frame, epsilon greedy algorithm is then used to select agent's action (exploration or exploitation) in the next frame. The outcome, action taken, current frame information and next frame information after taking the action, is then stored into the memory batch which, is used to update the neural network, allowing the agent to learn how to \"play the game\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966a61c",
   "metadata": {},
   "source": [
    "The \"assets\" folders and its contents, along with the \"src/flappy_bird.py\", are obtained from  this site: https://github.com/uvipen/Flappy-bird-deep-Q-learning-pytorch\n",
    "\n",
    "\"src/flappy_bird.py\" takes in a number, 0 means no action, 1 means jump, and returns 3 variables: state as image, reward as float, terminal as boolean. \n",
    "\n",
    "State is the image after action applied\n",
    "\n",
    "Reward is 0.1 if bird is alive, 1 if pass through pipe, -1 if dead.\n",
    "\n",
    "Terminal is TRUE if bird is dead, else FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff85a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from src.memory import *\n",
    "from src.convModel import *\n",
    "from src.utils import *\n",
    "from src.flappy_bird import *\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149927c",
   "metadata": {},
   "source": [
    "The train_model sub is used to train the agent in playing the game, it first selects the action by either randomly, or prediction from the neural network, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model will train model 2 million times and save model and graph every 100k times\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    # action to be taken by agent\n",
    "    # [1, 0] means no action\n",
    "    # [0, 1] means jump\n",
    "    action = torch.zeros(2, dtype=torch.float32)\n",
    "    \n",
    "    # used to keep count number of iterations passed (number of frames)\n",
    "    iter = 0\n",
    "\n",
    "    # getting initial state\n",
    "    state_image, reward, terminal = game.next_frame(action[1])\n",
    "    \n",
    "    # pre-processing on image, aiming to reduce image size and convert image to monochrome\n",
    "    state = pre_processing(state_image)\n",
    "    state = torch.cat((state, state, state, state)).unsqueeze(0)\n",
    "    alive_stat = []\n",
    "    alive_time = 0\n",
    "    log_path = \"tensorboard\"\n",
    "    save_path = \"trained_models\"\n",
    "    graph_path = \"graph\"\n",
    "    if os.path.isdir(log_path):\n",
    "        shutil.rmtree(log_path)\n",
    "    os.makedirs(log_path)\n",
    "    \n",
    "    # training the agent\n",
    "    while iter < MAX_ITER:\n",
    "        \n",
    "        iter = iter + 1\n",
    "        q_value = model(state)[0]\n",
    "        \n",
    "        action = torch.zeros(2, dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            action = action.cuda()\n",
    "        \n",
    "        # Epsilon-Greedy implementation\n",
    "        epsilon = INITIAL_EPSILON * (1 - iter / MAX_ITER)\n",
    "        u = random.random()\n",
    "        random_action = False if epsilon < u else True\n",
    "        index = torch.argmax(q_value).item()\n",
    "        \n",
    "        if random_action:\n",
    "            print(\"Performed random action!\")\n",
    "            index = randint(0, 1)     \n",
    "        action[index] = 1  \n",
    "\n",
    "        # Perform action and get next state information\n",
    "        next_state_image, reward, terminal = game.next_frame(action[1])\n",
    "        next_state_image = pre_processing(next_state_image)\n",
    "        next_state = torch.cat((state.squeeze(0)[1:, :, :], next_state_image)).unsqueeze(0)\n",
    "        \n",
    "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "        \n",
    "        action = action.unsqueeze(0)\n",
    "        \n",
    "        # save replay\n",
    "        memory.push(state, action, next_state, reward, terminal)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # update neural network\n",
    "        update_model()\n",
    "\n",
    "        # print information of each frame\n",
    "        print(\"Iteration: {}/{}, Action: {}, Reward: {}, Q-value: {}\".format(\n",
    "            iter + 1,\n",
    "            MAX_ITER,\n",
    "            action[0],\n",
    "            reward, torch.max(q_value)))\n",
    "\n",
    "        # saving model every 100000 iteration\n",
    "        alive_time += 1\n",
    "        if(iter+1) % 100000 == 0:\n",
    "            torch.save(model, \"{}/flappy_bird_{}\".format(save_path, iter+1))\n",
    "        \n",
    "        # plot how long agent survived\n",
    "        if terminal:\n",
    "            alive_stat.append(alive_time)\n",
    "            plot_duration(alive_stat)\n",
    "            alive_time = 0\n",
    "        \n",
    "        if(iter+1) % 100000 == 0:\n",
    "            plt.savefig('{}/train_{}.jpg'.format(graph_path, iter+1))\n",
    "    torch.save(model,\"{}/flappy_bird\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247d16f",
   "metadata": {},
   "source": [
    "This part updates the model by selecting a batch_size from memory, update the model using the memory along with the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587295b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model():\n",
    "    \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    # unpack minibatch\n",
    "    state_batch = torch.cat(tuple(d[0] for d in batch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in batch))\n",
    "    next_state_batch = torch.cat(tuple(d[2] for d in batch))\n",
    "    reward_batch = torch.cat(tuple(d[3] for d in batch))\n",
    "    terminal_batch =[d[4] for d in batch]\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        next_state_batch = next_state_batch.cuda()\n",
    "        terminal_batch = terminal_batch.cuda()\n",
    "\n",
    "    next_action_batch = model(next_state_batch)\n",
    "    # if dead, rj, otherwise r_j + gamma*max(Q_t+1)\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if batch[i][4]\n",
    "                                  else reward_batch[i] + DISCOUNT_FACTOR * torch.max(next_action_batch[i])\n",
    "                                  for i in range(len(batch))))\n",
    "    # Extract Q-value (this part i don't understand)\n",
    "    \n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(q_value, y_batch)\n",
    "\n",
    "    # Do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270da02",
   "metadata": {},
   "source": [
    "This part plots the time agent survived every 100 episodes, used for reporting purpose (see diagrams used above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42078bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duration(duration):\n",
    "    \"\"\"Plot durations of episodes and average over last 100 episodes\"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(duration, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec63cbe",
   "metadata": {},
   "source": [
    "We decided to alter the variables (Initial epsilon, learning rate, max experience, batch size) and compare the differences between each output, thus decides which is the best experience replay model:\n",
    "\n",
    "Training:\n",
    "\n",
    "![](train.PNG)\n",
    "\n",
    "Testing:\n",
    "\n",
    "![](test.PNG)\n",
    "\n",
    "It clearly shows that if we increase the memory size, the time taken to run the model is almost doubled, however agent is getting the same reward as default (left most image)\n",
    "If we increase the initial epsilon, i.e. increase the chance to take random action, the agent does not train at all\n",
    "If we decrease the learning rate however, the agent trains very well and the reward is 4 times higher!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48922e4",
   "metadata": {},
   "source": [
    "This is the main function to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-5\n",
    "MAX_ITER = 500000\n",
    "MAX_EXPERIENCE = 50\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "BATCH_SIZE = 30\n",
    "INITIAL_EPSILON = 0.2\n",
    "\n",
    "start_time = t.time()\n",
    "iter = 0\n",
    "\n",
    "# Constructing memory class\n",
    "memory = Memory(MAX_EXPERIENCE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "game = FlappyBird()\n",
    "model = ThreeLayerConvModel()\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "graph_path = \"graph\"\n",
    "plt.ion()\n",
    "train_model()\n",
    "print(t.time() - start_time)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "44b7ec50544db1c2794f193b6121fbe9a81e917175fa37135c38323ec59979e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
