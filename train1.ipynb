{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27586ebf",
   "metadata": {},
   "source": [
    "                                            Deep Q-learning for Flappy Bird\n",
    "Introduction/Background:\n",
    "\n",
    "This project aims to study and select the best model of deep-q-learning in training agent to play the game \"Flappy Bird\",by first comparing performance done by different model, then choosing the best model,experience replay, and optimise it by experimenting the reward received each episode under different hyperparameters used, ranging from initial epsilon, to learning rate, and memory size used to update the 3 layer convolution fully connected neural network. Agent first receive environment information through image per frame, epsilon greedy algorithm is then used to select agent's action (exploration or exploitation) in the next frame. The outcome, action taken, current frame information and next frame information after taking the action, is then stored into the memory batch which, is used to update the neural network, allowing the agent to learn how to \"play the game\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966a61c",
   "metadata": {},
   "source": [
    "Data sources(all detailed information and code can be found in train2.ipynb):\n",
    "\n",
    "Since there is no pre-existing data source for our project, our agent learns while playing the game and gets data generated from the environment at each time step as an image, then studies the bird position and pipe position. In other words, our agent is only given raw pixel information and will learn the representation of bird and pipes and interact with them. After performing an action, a new image, reward and a boolean of game terminated or not, is being generated.\n",
    "\n",
    "\n",
    "The game base we used include flappy_bird.py, which is where agent plays game, and images in asset folder. Detailed content is inside \"assets\" folder and \"src/flappy_bird.py\", and are obtained from  this site: https://github.com/uvipen/Flappy-bird-deep-Q-learning-pytorch\n",
    "\n",
    "\"src/flappy_bird.py\" takes in a number, 0 means no action, 1 means jump, and returns 3 variables: state as image, reward as float, terminal as boolean. \n",
    "\n",
    "State is the image after action applied\n",
    "\n",
    "Reward is 0.1 if bird is alive, 1 if pass through pipe, -1 if dead.\n",
    "\n",
    "Terminal is TRUE if bird is dead, else FALSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7950ec8",
   "metadata": {},
   "source": [
    "Data analysis and preprocess(all detailed information and code can be found in train2.ipynb):\n",
    "\n",
    "The resulting image from flappy_bird.py is put to pre_process.py and used as our input data to network after pre processing. The values at the final output layer represent the Q function given the input state for each valid action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff85a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from src.memory import *\n",
    "from src.convModel import *\n",
    "from src.utils import *\n",
    "from src.flappy_bird import *\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149927c",
   "metadata": {},
   "source": [
    "The train_model sub is used to train the agent in playing the game, it first selects the action by either randomly, or prediction from the neural network. This is done through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model will train model 2 million times and save model and graph every 100k times\n",
    "\"\"\"\n",
    "Method(s) or Model(s) (4 marks) :\n",
    "Make use of model (s), models are appropriate and clearly applied.\n",
    "\"\"\"\n",
    "def train_model():\n",
    "    \n",
    "    # action to be taken by agent\n",
    "    # [1, 0] means no action\n",
    "    # [0, 1] means jump\n",
    "    action = torch.zeros(2, dtype=torch.float32)\n",
    "    \n",
    "    # used to keep count number of iterations passed (number of frames)\n",
    "    iter = 0\n",
    "\n",
    "    # getting initial state\n",
    "    state_image, reward, terminal = game.next_frame(action[1])\n",
    "    \n",
    "    # pre-processing on image, aiming to reduce image size and convert image to monochrome\n",
    "    state = pre_processing(state_image)\n",
    "    state = torch.cat((state, state, state, state)).unsqueeze(0)\n",
    "    alive_stat = []\n",
    "    alive_time = 0\n",
    "    log_path = \"tensorboard\"\n",
    "    save_path = \"trained_models\"\n",
    "    graph_path = \"graph\"\n",
    "    if os.path.isdir(log_path):\n",
    "        shutil.rmtree(log_path)\n",
    "    os.makedirs(log_path)\n",
    "    \n",
    "    # training the agent\n",
    "    while iter < MAX_ITER:\n",
    "        \n",
    "        iter = iter + 1\n",
    "        q_value = model(state)[0]\n",
    "        \n",
    "        action = torch.zeros(2, dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            action = action.cuda()\n",
    "        \n",
    "        # Epsilon-Greedy implementation\n",
    "        epsilon = INITIAL_EPSILON * (1 - iter / MAX_ITER)\n",
    "        u = random.random()\n",
    "        random_action = False if epsilon < u else True\n",
    "        index = torch.argmax(q_value).item()\n",
    "        \n",
    "        if random_action:\n",
    "            print(\"Performed random action!\")\n",
    "            index = randint(0, 1)     \n",
    "        action[index] = 1  \n",
    "\n",
    "        # Perform action and get next state information\n",
    "        next_state_image, reward, terminal = game.next_frame(action[1])\n",
    "        # Pre_process image \n",
    "        next_state_image = pre_processing(next_state_image)\n",
    "        next_state = torch.cat((state.squeeze(0)[1:, :, :], next_state_image)).unsqueeze(0)\n",
    "        \n",
    "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "        \n",
    "        action = action.unsqueeze(0)\n",
    "        \n",
    "        # save replay\n",
    "        memory.push(state, action, next_state, reward, terminal)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # update neural network\n",
    "        update_model()\n",
    "\n",
    "        # print information of each frame\n",
    "        print(\"Iteration: {}/{}, Action: {}, Reward: {}, Q-value: {}\".format(\n",
    "            iter + 1,\n",
    "            MAX_ITER,\n",
    "            action[0],\n",
    "            reward, torch.max(q_value)))\n",
    "\n",
    "        # saving model every 100000 iteration\n",
    "        alive_time += 1\n",
    "        if(iter+1) % 100000 == 0:\n",
    "            torch.save(model, \"{}/flappy_bird_{}\".format(save_path, iter+1))\n",
    "        \n",
    "        # plot how long agent survived\n",
    "        if terminal:\n",
    "            alive_stat.append(alive_time)\n",
    "            plot_duration(alive_stat)\n",
    "            alive_time = 0\n",
    "        \n",
    "        if(iter+1) % 100000 == 0:\n",
    "            plt.savefig('{}/train_{}.jpg'.format(graph_path, iter+1))\n",
    "    torch.save(model,\"{}/flappy_bird\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247d16f",
   "metadata": {},
   "source": [
    "This part updates the model by selecting a batch_size from memory, update the model using the memory along with the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587295b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model():\n",
    "    \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    # unpack minibatch\n",
    "    state_batch = torch.cat(tuple(d[0] for d in batch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in batch))\n",
    "    next_state_batch = torch.cat(tuple(d[2] for d in batch))\n",
    "    reward_batch = torch.cat(tuple(d[3] for d in batch))\n",
    "    terminal_batch =[d[4] for d in batch]\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        next_state_batch = next_state_batch.cuda()\n",
    "        terminal_batch = terminal_batch.cuda()\n",
    "\n",
    "    next_action_batch = model(next_state_batch)\n",
    "    # if dead, rj, otherwise r_j + gamma*max(Q_t+1)\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if batch[i][4]\n",
    "                                  else reward_batch[i] + DISCOUNT_FACTOR * torch.max(next_action_batch[i])\n",
    "                                  for i in range(len(batch))))\n",
    "    # Extract Q-value (this part i don't understand)\n",
    "    \n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(q_value, y_batch)\n",
    "\n",
    "    # Do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270da02",
   "metadata": {},
   "source": [
    "This part plots the time agent survived every 100 episodes, used for reporting purpose (see diagrams used above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42078bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duration(duration):\n",
    "    \"\"\"Plot durations of episodes and average over last 100 episodes\"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(duration, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec63cbe",
   "metadata": {},
   "source": [
    "Result and Discussion:\n",
    "We have trained three models at start, plain 3-layer DQN(first image below),experience replay with 3-layer DQN(second image below), experience replay with DDQ network(third image below), and actor critic(A2C)(forth image below), and compared their performance based on rewards they get and time used to train.\n",
    "\n",
    "<img src=\"test_er.png\" width=\"250\"> \n",
    "<img src=\"DDQ.png\" width=\"250\"> \n",
    "<img src=\"A2c.png\" width=\"250\">  \n",
    "\n",
    "From analyzing the graph, we can see that experience replay with 3-layer DQN is trained successfully as the rewards it gains are increasing.\n",
    "DDQ does get a high reward and achieved this faster than our first method(reward graph converges earlier), but the rewards drops significantly and remains low after around 1600 episodes. From research and analysis, this might be caused by overfitting the Q function. We have been training our network for too long over data that are the same or very closely. In DDQ, we have two sets of weights w and w1, generated from two netwroks network1 and targetnetwork.One of them is used for action selection and the other for evaluation. To solve this problem(for further experiments),we could stop training after few hundreds episode and swap w and w1, or we can test out different hyper-parameters and implement a learning rate decay methods that suits. \n",
    "\n",
    "A2C on the third graph trained and reached the best performance at aorund 400 episodes, then the reward starts to decrease and the agent stop learning with a steady average mean reward.\n",
    "\n",
    "After the experiements, comparisons, and research, we've decided to use a 3-layer DQN experience replay model, which is more stable for taining purpose, faster, and have a generally good performance. However, we also noticed the importance of hyper-parameters to our model. So, in order to find the best performing model,we decided to alter the variables (Initial epsilon, learning rate, max experience, batch size) and compare the differences between each output, thus decides which is the best experience replay model:\n",
    "\n",
    "Training:\n",
    "\n",
    "![](train.PNG)\n",
    "\n",
    "Testing:\n",
    "\n",
    "![](test.PNG)\n",
    "\n",
    "It clearly shows that if we increase the memory size, the time taken to run the model is almost doubled, however agent is getting the same reward as default (left most image)\n",
    "If we increase the initial epsilon, i.e. increase the chance to take random action, the agent does not train at all\n",
    "If we decrease the learning rate however, the agent trains very well and the reward is 4 times higher!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48922e4",
   "metadata": {},
   "source": [
    "This is the main function to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "MAX_ITER = 500000\n",
    "MAX_EXPERIENCE = 50\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "BATCH_SIZE = 30\n",
    "INITIAL_EPSILON = 0.2\n",
    "\n",
    "start_time = t.time()\n",
    "iter = 0\n",
    "\n",
    "# Constructing memory class\n",
    "memory = Memory(MAX_EXPERIENCE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "game = FlappyBird()\n",
    "model = ThreeLayerConvModel()\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "graph_path = \"graph\"\n",
    "plt.ion()\n",
    "train_model()\n",
    "print(t.time() - start_time)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
