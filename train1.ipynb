{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27586ebf",
   "metadata": {},
   "source": [
    "<h1> Deep Q-learning for Flappy Bird </h1>\n",
    "    \n",
    "    \n",
    "    Introduction/Background:\n",
    "\n",
    "This project aims to study and select the best model of deep-q-learning in training agent to play the game \"Flappy Bird\", by first comparing performance done by different model, then choosing the best model, experience replay, and optimise it by experimenting the reward received each episode under different hyperparameters, ranging from initial epsilon, to learning rate, and memory size used to update the 3 layer convolution fully connected neural network. Agent first receive environment information through image per frame, epsilon greedy algorithm is then used to select agent's action (exploration or exploitation) in the next frame. The outcome, action taken, current frame information and next frame information after taking the action, is then stored into the memory batch which, is used to update the neural network, allowing the agent to learn how to \"play the game\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966a61c",
   "metadata": {},
   "source": [
    "    Data sources:\n",
    "\n",
    "There is no pre-existing data source in our project, instead our agent learns while playing the game and gets data generated from the environment at each time step as an image, then the agent studies the bird position and pipe position. In other words, our agent is only given raw pixel information and will learn the environment thus predicitng the best action to take. After performing an action, a new image, reward and a boolean of game terminated or not, is being generated.\n",
    "\n",
    "The game base we used include the game base *flappy_bird.py*, which is where the agent plays the game, and images in *assets\\sprites* folder, which helps to display the game. These are obtained from this site: https://github.com/uvipen/Flappy-bird-deep-Q-learning-pytorch, written by uvipen\n",
    "\n",
    "- The *next_frame()* funciton under *flappy_bird.py* takes in a number, 0 means no action, 1 means jump, and returns 3 variables: state as image, reward as float, terminal as boolean. \n",
    "\n",
    "- State is the image after action applied\n",
    "\n",
    "- Reward is 0.1 if bird is alive, 1 if pass through pipe, -1 if dead.\n",
    "\n",
    "- Terminal is TRUE if bird is dead, else FALSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7950ec8",
   "metadata": {},
   "source": [
    "    Data Analysis:\n",
    "\n",
    "The resulting image from flappy_bird.py is put to pre_process.py and used as our input data to network after pre processing. The values at the final output layer represent the Q values given the input state for each valid action.\n",
    "\n",
    "A short summary of important classes and variables:\n",
    "- Variables:\n",
    "    - LEARNING_RATE     --- learning rate $\\eta$, represents how important new data are in comparison to existing data when performing backward propagation\n",
    "    - MAX_ITER          --- max number of iteration the program runs\n",
    "    - MAX_EXPERIENCE    --- max number of experience stored\n",
    "    - DISCOUNT_FACTOR   --- discount rate of future rewards\n",
    "    - BATCH_SIZE        --- sample size when extracting batch from stored experiences\n",
    "    - INITIAL_EPSILON   --- initial epsilon, probability of choosing a random action, used in the epsilon-greedy algorithm\n",
    "    \n",
    "    - state             --- stored as an image\n",
    "    - next_state        --- stored as an image\n",
    "    - action            --- stored as [1, 0] (no jump) or [0, 1] (jump)\n",
    "    - reward            --- stored as float (0.1 or 1 or -1)\n",
    "    - terminal          --- stored as boolean, bird dead = TRUE, else FALSE\n",
    "- Classes:\n",
    "    - *ThreeLayerConvModel* --- stores the 3 layer convolutional fully connected neural network\n",
    "    \n",
    "    - *Memory* --- stores the most recent gameplay state, reward, terminal into the class, if exceeds MAX_EXPERIENCE, drop the oldest memory\n",
    "    - *FlappyBird* --- game base which takes in a value (action) and returns state, reward, terminal, written by uvipen (refer to link above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff85a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from src.memory import *\n",
    "from src.convModel import *\n",
    "from src.utils import *\n",
    "from src.flappy_bird import *\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149927c",
   "metadata": {},
   "source": [
    "    Methods: \n",
    "The *train_model()* sub is used to train the agent in playing the game\n",
    "\n",
    "Steps:\n",
    "1.  Generate initial state (get starting image by passing in *action = 0* into *game.next_frame()*)\n",
    "2.  Perform random action or use prediction from the neural network, with the aids of epsilon greedy algorithm.\n",
    "3.  Epsilon greedy algorithm has initial epsilon of 0.2, meaning initially 20% of the chance to select a random action (exploration). As the number of iteration increases, the chance the select a random action decreases linearly \n",
    "$$\\text{new epsilon} = \\text{initial epsilon} \\times (1 - \\frac{\\text{number of iterations passed}}{\\text{MAX ITERATION}})$$\n",
    "4.  The action generated in step 3 is then inputted into *game.next_frame(action)*, which gives information about next frame: next_state, reward, terminal\n",
    "5.  These information are then stored into class *Memory*, and the model is then updated (see below for detailed breakdown)\n",
    "6.  A model is saved every 100000 iterations, and a graph is printed everytime the bird dies, showing how long the bird survived each gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model will train model 2 million times and save model and graph every 100k times\n",
    "\"\"\"\n",
    "Method(s) or Model(s) (4 marks) :\n",
    "Make use of model (s), models are appropriate and clearly applied.\n",
    "\"\"\"\n",
    "def train_model():\n",
    "    \n",
    "    # action to be taken by agent\n",
    "    # [1, 0] means no action\n",
    "    # [0, 1] means jump\n",
    "    action = torch.zeros(2, dtype=torch.float32)\n",
    "    \n",
    "    # used to keep count number of iterations passed (number of frames)\n",
    "    iter = 0\n",
    "\n",
    "    # getting initial state\n",
    "    state_image, reward, terminal = game.next_frame(action[1])\n",
    "    \n",
    "    # pre-processing on image, aiming to reduce image size and convert image to monochrome\n",
    "    state = pre_processing(state_image)\n",
    "    state = torch.cat((state, state, state, state)).unsqueeze(0)\n",
    "    alive_stat = []\n",
    "    alive_time = 0\n",
    "    save_path = \"trained_models\"\n",
    "    graph_path = \"graph\"\n",
    "    \n",
    "    # training the agent\n",
    "    while iter < MAX_ITER:\n",
    "        \n",
    "        iter = iter + 1\n",
    "        q_value = model(state)[0]\n",
    "        \n",
    "        action = torch.zeros(2, dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            action = action.cuda()\n",
    "        \n",
    "        # Epsilon-Greedy implementation\n",
    "        epsilon = INITIAL_EPSILON * (1 - iter / MAX_ITER)\n",
    "        u = random.random()\n",
    "        random_action = False if epsilon < u else True\n",
    "        index = torch.argmax(q_value).item()\n",
    "        \n",
    "        if random_action:\n",
    "            print(\"Performed random action!\")\n",
    "            index = randint(0, 1)     \n",
    "        action[index] = 1  \n",
    "\n",
    "        # Perform action and get next state information\n",
    "        next_state_image, reward, terminal = game.next_frame(action[1])\n",
    "        # Pre_process image \n",
    "        next_state_image = pre_processing(next_state_image)\n",
    "        next_state = torch.cat((state.squeeze(0)[1:, :, :], next_state_image)).unsqueeze(0)\n",
    "        \n",
    "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "        \n",
    "        action = action.unsqueeze(0)\n",
    "        \n",
    "        # save replay\n",
    "        memory.push(state, action, next_state, reward, terminal)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # update neural network\n",
    "        update_model()\n",
    "\n",
    "        # print information of each frame\n",
    "        print(\"Iteration: {}/{}, Action: {}, Reward: {}, Q-value: {}\".format(\n",
    "            iter + 1,\n",
    "            MAX_ITER,\n",
    "            action[0],\n",
    "            reward, torch.max(q_value)))\n",
    "\n",
    "        # saving model every 100000 iteration\n",
    "        alive_time += 1\n",
    "        if(iter+1) % 100000 == 0:\n",
    "            torch.save(model, \"{}/flappy_bird_{}\".format(save_path, iter+1))\n",
    "        \n",
    "        # plot how long agent survived\n",
    "        if terminal:\n",
    "            alive_stat.append(alive_time)\n",
    "            plot_duration(alive_stat)\n",
    "            alive_time = 0\n",
    "        \n",
    "        if(iter+1) % 100000 == 0:\n",
    "            plt.savefig('{}/train_{}.jpg'.format(graph_path, iter+1))\n",
    "    torch.save(model,\"{}/flappy_bird\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247d16f",
   "metadata": {},
   "source": [
    "This part updates the model by selecting a batch_size from memory, update the model using the memory along with the loss function\n",
    "\n",
    "Steps:\n",
    "1.  Retrieve a list of memory from *Memory* class with size min(length of memory, BATCH_SIZE), where BATCH_SIZE is a specified constant\n",
    "2.  Compute loss function:\n",
    "$$[r_t + \\gamma \\max_b{Q_w(s_{t+1}, b)} - Q_w(s_t, a_t)]^2 ---- (1)$$\n",
    "3.  The *y_batch* computes the $r_t + \\gamma \\max_b{Q_w(s_{t+1}, b)}$ part of the loss function, however since there is a possibility of bird dying and game terminates, there we consider if terminal = TRUE, we only use $r_t$\n",
    "4.  The *q_value* computes the $Q_w(s_t, a_t)$ part\n",
    "5.  The loss function is then calculated by *loss = criterion(y_batch, q_value)*, with criterion set as Mean-Squared Error, or mathematically:\n",
    "$$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\eta \\times (1)$$\n",
    "$$ \\text{where } \\eta \\text{ is learning rate} $$\n",
    "6.  Model is then updated based on the loss function using backward propagation *loss.backward()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587295b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model():\n",
    "    \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    # unpack batch\n",
    "    state_batch = torch.cat(tuple(d[0] for d in batch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in batch))\n",
    "    next_state_batch = torch.cat(tuple(d[2] for d in batch))\n",
    "    reward_batch = torch.cat(tuple(d[3] for d in batch))\n",
    "    terminal_batch =[d[4] for d in batch]\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        next_state_batch = next_state_batch.cuda()\n",
    "        terminal_batch = terminal_batch.cuda()\n",
    "\n",
    "    next_action_batch = model(next_state_batch)\n",
    "    \n",
    "    # if dead, rj, otherwise r_j + gamma*max(Q_t+1)\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if terminal_batch[i]\n",
    "                                  else reward_batch[i] + DISCOUNT_FACTOR * torch.max(next_action_batch[i])\n",
    "                                  for i in range(len(batch))))\n",
    "\n",
    "    \n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Detaching from the current graph so that it won't causes error in next iteration\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(q_value, y_batch)\n",
    "\n",
    "    # Do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270da02",
   "metadata": {},
   "source": [
    "This part plots the time agent survived every 100 episodes, used for reporting purpose (see diagrams used below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42078bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duration(duration):\n",
    "    # Plot durations of episodes and average over last 100 episodes\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(duration, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f743f47",
   "metadata": {},
   "source": [
    "*test()* plays the game by using the trained model. A graph is being plotted and updated each time the bird dies, with y value as number of frames bird stays alive, and x value as number of games played. For every 100000 iteration, the graph is saved in jpg file format under folder *graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test will test 100000 iter and save the graph\n",
    "\n",
    "def test():\n",
    "    save_path = \"trained_models\"\n",
    "    # Modify the model you want to test, keep \"_\" \n",
    "    model_number = \"_100000\"\n",
    "    model = torch.load(\"{}/flappy_bird{}\".format(save_path, model_number))\n",
    "    model.eval()\n",
    "    \n",
    "    # Play the game using the trained model\n",
    "    action = torch.zeros(2, dtype=torch.float32)\n",
    "    state_image, reward, terminal = game.next_frame(action[1])\n",
    "    state = pre_processing(state_image)\n",
    "    state = torch.cat((state, state, state, state)).unsqueeze(0)\n",
    "    alive_time = 0\n",
    "    iter = 0\n",
    "    graph_path = \"graph\"\n",
    "    alive_stat = []\n",
    "    while iter < MAX_ITER:\n",
    "        q_value = model(state)[0]\n",
    "        action = torch.argmax(q_value, 0)\n",
    "        next_state_image, reward, terminal = game.next_frame(action)\n",
    "        next_state_image = pre_processing(next_state_image)\n",
    "        next_state = torch.cat((state.squeeze(0)[1:, :, :], next_state_image)).unsqueeze(0)\n",
    "        state = next_state\n",
    "        \n",
    "        alive_time += 1\n",
    "        \n",
    "        # Add in data (how many frame bird survived) to the plot\n",
    "        if terminal:\n",
    "            alive_stat.append(alive_time)\n",
    "            plot_duration(alive_stat)\n",
    "            alive_time = 0\n",
    "        \n",
    "        # Graph saved\n",
    "        if(iter+1) % 100000 == 0:\n",
    "            plt.savefig('{}/test{}.jpg'.format(graph_path, model_number))\n",
    "        iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec63cbe",
   "metadata": {},
   "source": [
    "    Result and Discussion:\n",
    "Three models had been trained at the beginning to decide which one is the better approach\n",
    "1.  Q-learning with 3-layer Convolutional Fully Connected Network (Q-learning is trained by setting MAX_EXPERIENCE and BATCH_SIZE as 1)\n",
    "2.  Deep Q-Leraning Experience Replay with 3-layer Convolutional Fully Connected Network\n",
    "3.  Double Q-Learning with 3-layer Convolutional Fully Connected Network\n",
    "\n",
    "Their performance is then compared based on rewards they get and time used to train.\n",
    "\n",
    "Q-Learning: 3 Layer Q-Learning\n",
    "\n",
    "<img src=\"graph/train_100000.jpg\" width=\"300\"> \n",
    "\n",
    "Deep Q-Learning (Experience Replay): 3 Layer Convolutional Fully Connected Network \n",
    "\n",
    "<img src=\"train_norm.png\" width=\"300\"> \n",
    "\n",
    "Double Q-Learning: 3 Layer Convolutional Fully Connected Network\n",
    "\n",
    "<img src=\"DDQ.png\" width=\"300\">\n",
    "\n",
    "By analyzing the graph, we can see that experience replay is trained successfully as the reward it gains are increasing.\n",
    "\n",
    "Double Q-Learning does get a high reward and achieved this faster than our first method(reward graph converges earlier), but the rewards drops significantly and remains low after around 1600 episodes. From research and analysis, this might be caused by overfitting the Q function. In this secenario, we have two sets of weights w and w1, generated from two netwroks network1 and targetnetwork. One of them is used for action selection and the other for evaluation. To solve this problem(for further experiments), we could test out different hyper-parameters and implement a learning rate decay methods that suits.\n",
    "\n",
    "After the experiements, comparisons, and research, we've decided to use a 3-layer DQN experience replay model, which is more stable for taining purpose, faster, and have a generally good performance. However, we also noticed the importance of hyper-parameters to our model. So, in order to find the best performing model,we decided to alter the variables (Initial epsilon, learning rate, max experience, batch size) and compare the differences between each output, thus decides which is the best experience replay model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d65a6",
   "metadata": {},
   "source": [
    "Since the performance of experience replay is affected by some predetermined parameters, we decided to test out how each parameter affects the model performanc:\n",
    "\n",
    "Training:\n",
    "\n",
    "<img src=\"train.PNG\" width=\"1500\"> \n",
    "\n",
    "Testing:\n",
    "\n",
    "<img src=\"test.PNG\" width=\"1500\">\n",
    "\n",
    "*(For ease of analysis, we refer left most scenario as Scenario 1, and right most scenario as Scenario 4)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03432d02",
   "metadata": {},
   "source": [
    "*For analysis below, we will use Scenario 1 as the base scenario, and compare it against Scenario 2 - 4*\n",
    "\n",
    "For the training diagram, we can see under Scenario 1 to Scenario 3, agents tend to train successfully, whereas for Scenario 4, agent fails to train. We believe Scenario 4 agent fails to train because the initial epsilon is too high, there is a 60% chance for agent to choose a random action!\n",
    "\n",
    "Under Scenario 3, agent trains successfully however, the time taken to pass through 3000 episdoes (games) is almost doubled comparing to Scenario 1 and 2. This is because the model is updated every iteration and, each iteration we have to re-extract a much larger sample size from the stored memories thus, it requires more time per iteration.\n",
    "\n",
    "Under Scenario 2, the agent seems to be training slightly better than the base secenario towards the end (roughly 2000+ episodes), however simply from the training diagram there are no strong indicators suggesting either model outperforms one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29496c7c",
   "metadata": {},
   "source": [
    "\n",
    "However, the testing diagram suggests a different result:\n",
    "\n",
    "From the testing diagram, Scenario 4 is blanked out because the program crashed halfway as such, it is safe to conclude that Scenario 4 is not a good approach.\n",
    "\n",
    "Scenario 3 does have a higher maximum reward obtained comparing to base scenario however, if we refer to the average line (orange line), both models have very similar average reward. The reason being is likely that there is already sufficient memory size and sample size taken under the base scenario, increasing both size will not affect the average reward significantly however, increases the time taken drastically\n",
    "\n",
    "Scenario 2 on the other hand, shows that the agent is able to learn the environment very well, with the average reward roughly being 4 times higher than the base scenario! This is likely because the learning rate in the base scenario is too high that it is overfitting the model, as such it is safe to conclude that scenario 2 is the best model to use for deep q learning in this environment\n",
    "\n",
    "To conclude, Experience replay generally is a good approach in reinforced learning however, the main weakness is that correct hyperparameters need to be chosen, and the correct hyperparameters can only be chosen after multiple trial and errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48922e4",
   "metadata": {},
   "source": [
    "This is the code to train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "MAX_ITER = 500000\n",
    "MAX_EXPERIENCE = 50\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "BATCH_SIZE = 30\n",
    "INITIAL_EPSILON = 0.2\n",
    "\n",
    "start_time = t.time()\n",
    "iter = 0\n",
    "\n",
    "# Constructing memory class\n",
    "memory = Memory(MAX_EXPERIENCE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "game = FlappyBird()\n",
    "model = ThreeLayerConvModel()\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "graph_path = \"graph\"\n",
    "plt.ion()\n",
    "train_model()\n",
    "print(t.time() - start_time)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80518bc2",
   "metadata": {},
   "source": [
    "This is the code to test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 100000\n",
    "iter = 0\n",
    "game = FlappyBird()\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "test()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "44b7ec50544db1c2794f193b6121fbe9a81e917175fa37135c38323ec59979e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
